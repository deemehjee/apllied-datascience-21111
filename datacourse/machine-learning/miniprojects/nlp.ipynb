{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/nlp.py append\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Miniproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The objective of this miniproject is to gain experience with natural language processing and how to use text data to train a machine learning model to make predictions. For the miniproject, we will be working with product review text from Amazon. The reviews are for only products in the \"Electronics\" category. The objective is to train a model to predict the rating, ranging from 1 to 5 stars.\n",
    "\n",
    "## Scoring\n",
    "\n",
    "For most of the questions, you will be asked to submit the `predict` method of your trained model to the grader. The grader will use the passed `predict` method to evaluate how your model performs on a test set with respect to a reference model. The grader uses the [R<sup>2</sup>-score](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score) for model evaluation. If your model performs better than the reference solution, then you can score higher than 1.0. For the last question, you will submit the results of an analysis and your passed answer will be compared directly to the reference solution.\n",
    "\n",
    "## Downloading and loading the data\n",
    "\n",
    "The data set is available on Amazon S3 and comes as a compressed file where each line is a JSON object. To load the data set, we will need to use the `gzip` library to open the file and decode each JSON into a Python dictionary. In the end, we have a list of dictionaries, where each dictionary represents an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "File ‘./data/amazon_electronics_reviews_training.json.gz’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir data\n",
    "wget http://dataincubator-wqu.s3.amazonaws.com/mldata/amazon_electronics_reviews_training.json.gz -nc -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import simplejson as json\n",
    "\n",
    "with gzip.open(\"data/amazon_electronics_reviews_training.json.gz\", \"r\") as f:                                  \n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings are stored in the keyword `\"overall\"`. You should create an array of the ratings for each review, preferably using list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [row['overall'] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.226383333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ratings)/len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, the test set used by the grader is in the same format as that of `data`, a list of dictionaries. Your trained model needs to accept data in the same format. Thus, you should use `Pipeline` when constructing your model so that all necessary transformation needed are encapsulated into a single estimator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Bag of words model\n",
    "\n",
    "Construct a machine learning model trained on word counts using the bag of words algorithm. Remember, the bag of words is implemented with `CountVectorizer`. Some things you should consider:\n",
    "\n",
    "* The reference solution uses a linear model and you should as well; use either `Ridge` or `SGDRegressor`.\n",
    "* The text review is stored in the key `\"reviewText\"`. You will need to construct a custom transformer to extract out the value of this key. It will be the first step in your pipeline.\n",
    "* Consider what hyperparameters you will need to tune for your model.\n",
    "* Subsampling the training data will boost training times, which will be helpful when determining the best hyperparameters to use. Note, your final model will perform best if it is trained on the full data set.\n",
    "* Including stop words may help with performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeySelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col):\n",
    "        self.col=col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return [row[self.col] for row in X]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I bought this mouse to use with my laptop because I don't like those little touchpads.  I could not be happier.Since it's USB, I can plug it in with the computer already on and expect it to work automatically.  Since it's optical (the new kind, not to be confused with the old Sun optical mice that required a special checkered mouse pad) it works on most surfaces, including my pant legs, my couch, and random tables that I put my laptop down on.  It's also light and durable, features that help with portability.The wheel is surprisingly useful.  In addition to scrolling, it controls zoom and pan in programs like Autocad and 3D Studio Max.  I can no longer bear using either of these programs without it.One complaint - the software included with the Internet navigation features is useless.  Don't bother installing it if you have a newer Windows version that automatically supports wheel mice.  Just plug it in and use it - it's that easy.\",\n",
       " 'One by one, all of the discs went bad within a 6 months period. It was a real pain. As a result, I would tend to not buy Memorex discs again.',\n",
       " 'Easy to install.  Works well.  No complaints.  Decent (not great) value.  Works better than the first amplifier (different brand) that I purchased.',\n",
       " 'This item was exactly what I was looking for.  I purchased a low featured samsung surround sound system but then realized I needed more optical inputs to take advantage of it.  The manual turn switches looked cheap and unusable.  This was perfect being able to remotely control it.Definitely worth every dime spent on it.',\n",
       " 'I purchased this to replace my vcr dvd combo. because I had no way to run cable out of the wall to RCA  for the rest of my system. this box will not do it. this box will work with RCA to RCA  not coax to RCA. I have the same problem, no way to connect to the wall cable. None of the newer dvd or vcr  will take coax cable in.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector=KeySelector(\"reviewText\")\n",
    "selector.fit_transform(data, ratings)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "bag_of_words_model = Pipeline([\n",
    "    ('selector', selector),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('regressor', Ridge(alpha=200)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of alpha ridge has to be edited to 100 and above to get a good score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, ratings, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer', CountVectorizer()),\n",
       "                ('regressor', Ridge(alpha=200))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer', CountVectorizer()),\n",
       "                ('regressor', Ridge(alpha=200))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_model.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.353821031864136"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bag_of_words_model[-1].coef_**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_of_words_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.888\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__bag_of_words_model(bag_of_words_model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Normalized model\n",
    "\n",
    "Using raw counts will not be as effective compared if we had normalized the counts. There are several ways to normalize raw counts; the `HashingVectorizer` class has the keyword `norm` and there is also the `TfidfTransformer` and `TfidfVectorizer` that perform tf-idf weighting on the counts. Apply normalization to your model to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLength(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([len(x) for x in X]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_length_model = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('compute_length', CharacterLength()),\n",
    "    ('regressor', Ridge()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('compute_length', CharacterLength()), ('regressor', Ridge())])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_length_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('compute_length', CharacterLength()), ('regressor', Ridge())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_length_model.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.013\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__normalized_model(character_length_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_length_model = Pipeline([\n",
    "    ('selector', selector),\n",
    "    ('compute_length', CharacterLength()),\n",
    "    ('regressor', Ridge(alpha=1000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('compute_length', CharacterLength()),\n",
       "                ('regressor', Ridge(alpha=1000))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_length_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.010\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__normalized_model(character_length_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score is 0 because it is a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_model = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('vectorizer', HashingVectorizer()),\n",
    "    ('regressor', Ridge()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=normalized_model[:-1].fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer', HashingVectorizer()), ('regressor', Ridge())])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalized_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.015\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__normalized_model(normalized_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with 1st, obviously normalized model is a better model. \n",
    "grader.score.nlp__bag_of_words_model(normalized_model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Bigrams model\n",
    "\n",
    "The model performance may increase when including additional features generated by counting bigrams. Include bigrams to your model. When using more features, the risk of overfitting increases. Make sure you try to minimize overfitting as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigrams_model = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('vectorizer', HashingVectorizer(ngram_range=(1,2))),\n",
    "    ('regressor', Ridge()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer', HashingVectorizer(ngram_range=(1, 2))),\n",
       "                ('regressor', Ridge())])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.128\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__bigrams_model(bigrams_model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Polarity analysis\n",
    "\n",
    "Let's derive some insight from our analysis. We want to determine the most polarizing words in the corpus of reviews. In other words, we want identify words that strongly signal a review is either positive or negative. For example, we understand a word like \"terrible\" will mostly appear in negative rather than positive reviews. The naive Bayes model calculates probabilities such as $P(\\text{terrible } | \\text{ negative})$, the probability the word \"terrible\" appears in the text, given that the review is negative. Using these probabilities, we can derive a **polarity score** for each counted word,\n",
    "\n",
    "$$\n",
    "\\text{polarity} =  \\log\\left(\\frac{P(\\text{word } | \\text{ positive})}{P(\\text{word } | \\text{ negative})}\\right).\n",
    "$$ \n",
    "\n",
    "The polarity analysis is an example where a simpler model offers more explicability than a more complicated model. For this question, you are asked to determine the top thirty words with the largest positive **and** largest negative polarity, for a total of sixty words. For this analysis, you should:\n",
    "\n",
    "1. Use the naive Bayes model, `MultinomialNB`.\n",
    "1. Use tf-idf weighting.\n",
    "1. Remove stop words.\n",
    "\n",
    "A trained naive Bayes model stores the log of the probabilities in the attribute `feature_log_prob_`. It is a NumPy array of shape (number of classes, the number of features). You will need the mapping between feature index to word. For this problem, you will use a different data set; it has been processed to only include reviews with one and five stars. You can download it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File ‘./data/amazon_one_and_five_star_reviews.json.gz’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://dataincubator-wqu.s3.amazonaws.com/mldata/amazon_one_and_five_star_reviews.json.gz -nc -P ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid memory issues, let's delete the older data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "with gzip.open(\"data/amazon_one_and_five_star_reviews.json.gz\", \"r\") as f:\n",
    "    data_polarity = [json.loads(line) for line in f]\n",
    "\n",
    "ratings = [row['overall'] for row in data_polarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS)),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', ...})),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use this to fit: model.fit(data_polarity, ratings)\n",
    "\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', ...})),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_polarity, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7061141 , -0.33401936, -0.29165675, ...,  0.61954703,\n",
       "        0.10706008,  0.19475051])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity=model[-1].feature_log_prob_[0, :] - model[-1].feature_log_prob_[1, :]\n",
    "\n",
    "polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3469003 , -2.33051588, -2.18552209, ...,  2.87344209,\n",
       "        2.96722437,  3.13553283])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11288,  3610, 17646, ..., 19021, 24531, 18497])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11288,  3610, 17646, 16559, 14790, 17123,  2511, 21610, 13921,\n",
       "       11980,  8880,  4036, 16917,   524,  5010, 10947,  3303, 17124,\n",
       "        7961,  6376, 16718, 22331,  4353,  9267, 18541,  8197,  2228,\n",
       "       16535, 13763, 21903])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(polarity)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3305158753579107"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity[3610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_most_polar=np.hstack((np.argsort(polarity)[:30], np.argsort(polarity)[-30:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_most_polar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly\n",
      "beat\n",
      "protects\n",
      "perfect\n",
      "monopod\n",
      "portrait\n",
      "amazing\n",
      "sturdy\n",
      "macro\n",
      "incredible\n",
      "excellent\n",
      "bokeh\n",
      "pleased\n",
      "200mm\n",
      "charm\n",
      "handy\n",
      "awesome\n",
      "portraits\n",
      "dslr\n",
      "crisp\n",
      "photography\n",
      "telephoto\n",
      "buck\n",
      "fantastic\n",
      "regrets\n",
      "easy\n",
      "affordable\n",
      "penny\n",
      "loves\n",
      "surround\n",
      "dead\n",
      "poorly\n",
      "send\n",
      "sent\n",
      "contacted\n",
      "refused\n",
      "threw\n",
      "disappointing\n",
      "randomly\n",
      "stopped\n",
      "unreliable\n",
      "horrible\n",
      "awful\n",
      "unacceptable\n",
      "poor\n",
      "beware\n",
      "defective\n",
      "trash\n",
      "worse\n",
      "worthless\n",
      "useless\n",
      "garbage\n",
      "returned\n",
      "terrible\n",
      "junk\n",
      "worst\n",
      "returning\n",
      "return\n",
      "waste\n",
      "refund\n"
     ]
    }
   ],
   "source": [
    "for index in ind_most_polar:\n",
    "    print(model[1].get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60 = [model[1].get_feature_names()[index] for index in ind_most_polar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.nlp__most_polar(top_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Topic modeling [optional]\n",
    "\n",
    "Topic modeling is the analysis of determining the key topics or themes in a corpus. With respect to machine learning, topic modeling is an unsupervised technique. One way to uncover the main topics in a corpus is to use [non-negative matrix factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html). For this question, use non-negative matrix factorization to determine the top ten words for the first twenty topics. You should submit your answer as a list of lists. What topics exist in the reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    " \n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "with gzip.open(\"data/amazon_one_and_five_star_reviews.json.gz\", \"r\") as f:\n",
    "    data_polarity = [json.loads(line) for line in f]\n",
    "\n",
    "ratings = [row['overall'] for row in data_polarity]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.update({\"ll\", \"ve\"})\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This worked perfectly for about 8 rewinds.  But once it eats one of your precious tapes you'll probably be as dissapointed as I was.  Buyer beware... these rewinders are mostly the same look; but marketed by different companies.  They all eat tape.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_polarity[0]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "work\n",
      "perfectly\n",
      "for\n",
      "about\n",
      "8\n",
      "rewind\n",
      ".\n",
      " \n",
      "but\n",
      "once\n",
      "-PRON-\n",
      "eat\n",
      "one\n",
      "of\n",
      "-PRON-\n",
      "precious\n",
      "tape\n",
      "-PRON-\n",
      "will\n",
      "probably\n",
      "be\n",
      "as\n",
      "dissapointed\n",
      "as\n",
      "-PRON-\n",
      "be\n",
      ".\n",
      " \n",
      "Buyer\n",
      "beware\n",
      "...\n",
      "these\n",
      "rewinder\n",
      "be\n",
      "mostly\n",
      "the\n",
      "same\n",
      "look\n",
      ";\n",
      "but\n",
      "market\n",
      "by\n",
      "different\n",
      "company\n",
      ".\n",
      " \n",
      "-PRON-\n",
      "all\n",
      "eat\n",
      "tape\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(data_polarity[0]['reviewText']):\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(document):\n",
    "    return [word.lemma_.lower() for word in nlp(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_LEMMA = {word.lemma_.lower() for word in nlp(\" \".join(STOP_WORDS))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\",\n",
       " \"'d\",\n",
       " \"'s\",\n",
       " '-pron-',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'because',\n",
       " 'become',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'could',\n",
       " 'd',\n",
       " 'do',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'have',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'm',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'meanwhile',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regard',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seeming',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'use',\n",
       " 'various',\n",
       " 've',\n",
       " 'very',\n",
       " 'via',\n",
       " 'well',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'your',\n",
       " 'yourself',\n",
       " '’d'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS_LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topic = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS_LEMMA, tokenizer=lemmatize)),\n",
    "    ('dim-reduction', NMF(n_components=20, random_state=0)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={\"'\", \"'d\", \"'s\", '-pron-', 'a',\n",
       "                                             'about', 'above', 'across',\n",
       "                                             'after', 'afterwards', 'again',\n",
       "                                             'against', 'all', 'almost',\n",
       "                                             'alone', 'along', 'already',\n",
       "                                             'also', 'although', 'always', 'am',\n",
       "                                             'among', 'amongst', 'amount', 'an',\n",
       "                                             'and', 'another', 'any', 'anyhow',\n",
       "                                             'anyone', ...},\n",
       "                                 tokenizer=<function lemmatize at 0x7fa27b59a040>)),\n",
       "                ('dim-reduction', NMF(n_components=20, random_state=0))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_topic.fit(data_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selector': KeySelector(col='reviewText'),\n",
       " 'vectorizer': TfidfVectorizer(stop_words={\"'\", \"'d\", \"'s\", '-pron-', 'a', 'about', 'above',\n",
       "                             'across', 'after', 'afterwards', 'again', 'against',\n",
       "                             'all', 'almost', 'alone', 'along', 'already',\n",
       "                             'also', 'although', 'always', 'am', 'among',\n",
       "                             'amongst', 'amount', 'an', 'and', 'another', 'any',\n",
       "                             'anyhow', 'anyone', ...},\n",
       "                 tokenizer=<function lemmatize at 0x7fa27b59a040>),\n",
       " 'dim-reduction': NMF(n_components=20, random_state=0)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_topic.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. find the indices that contribute the most to each feature\n",
    "#2. find the words associated with each feature\n",
    "#3. print out the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-c20b30f9f893>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-c20b30f9f893>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    nmf= model.named_steps[\"dim-reduction\"]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def find_ind_biggest(model, N, topic_number):\n",
    "   # \"\"\"\n",
    "    #Returns the indices that most contribute to a feature\n",
    "     # model: trained ML model\n",
    "   # N: The number of top words\n",
    "   # topic_number: new feature index\n",
    "   # \"\"\"\n",
    "    nmf= model.named_steps[\"dim-reduction\"]\n",
    "    #return np.argsort(-nmf.components_[topic_number, :])[:N]\n",
    "\n",
    "def find_ind_biggest(vector, num_words_topic):\n",
    "    return np.argsort(-vector)[:N]\n",
    "\n",
    "\n",
    "def find_words(model, ind):\n",
    "    vectorizer= model.named_steps[\"vectorizer\"]\n",
    "\n",
    "    features= vectorizer.get_feature_names()\n",
    "\n",
    "    return [features[i] for i in ind]\n",
    "\n",
    "\n",
    "    \n",
    "def print_result(words, topic_number):\n",
    "    print (f\" Top words for topic: {topic_number}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    for word in words:\n",
    "        print(word)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def find_words_of_topic(model, vector, topic_number, num_words_topic=20):\n",
    "    ind= find_ind_biggest(vector, num_words_topic)\n",
    "    words= find_words(model, ind)\n",
    "    print_result(words, topic_number)\n",
    "    \n",
    "#def find_words_of_topic(model, topic_number, N=20):\n",
    "    #ind= find_ind_biggest(model, N, topic_number)\n",
    "   # words= find_words(model, ind)\n",
    "   # print_result(words)\n",
    "    \n",
    "    \n",
    "def main(model, num_words_topic=20):\n",
    "    for i, vector in enumerate(model.named_steps[\"dim-reduction\"].components_):\n",
    "        find_words_of_topic(model, vector, i, num_words_topic=num_words_topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NMF' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-1b5636604742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-b7eab02a655a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, num_words_topic)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words_topic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dim-reduction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mfind_words_of_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words_topic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NMF' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "main(model_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cable',\n",
       " '.',\n",
       " 'connect',\n",
       " 'tv',\n",
       " 'need',\n",
       " 'connector',\n",
       " 'length',\n",
       " 'quality',\n",
       " 'belkin',\n",
       " 'monitor',\n",
       " 'long',\n",
       " 'signal',\n",
       " 'monster',\n",
       " 'foot',\n",
       " 'order',\n",
       " 'video',\n",
       " 'hook',\n",
       " 'short',\n",
       " 'extension',\n",
       " 'connection']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind=find_ind_biggest(model_topic, 20, 6)\n",
    "find_words(model_topic, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "with gzip.open(\"data/amazon_one_and_five_star_reviews.json.gz\", \"r\") as f:\n",
    "    data_polarity = [json.loads(line) for line in f]\n",
    "\n",
    "ratings = [row['overall'] for row in data_polarity]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.update({\"ll\", \"ve\"})\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This worked perfectly for about 8 rewinds.  But once it eats one of your precious tapes you'll probably be as dissapointed as I was.  Buyer beware... these rewinders are mostly the same look; but marketed by different companies.  They all eat tape.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_polarity[0]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeySelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col):\n",
    "        self.col=col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return [row[self.col] for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ind_biggest(model, N, topic_number):\n",
    "    \"\"\"\n",
    "       Returns the indices that most contribute to a feature\n",
    "       model: trained ML model\n",
    "       N: The number of top words\n",
    "       topic_number: new feature index\n",
    "     \"\"\"\n",
    "    nmf= model.named_steps[\"dim-reduction\"]\n",
    "    return np.argsort(-nmf.components_[topic_number, :])[:N]\n",
    "\n",
    "\n",
    "\n",
    "def find_words(model, ind):\n",
    "    vectorizer= model.named_steps[\"vectorizer\"]\n",
    "\n",
    "    features= vectorizer.get_feature_names()\n",
    "\n",
    "    return [features[i] for i in ind]\n",
    "\n",
    "\n",
    "    \n",
    "def print_result(words, topic_number):\n",
    "    print (f\" Top words for topic: {topic_number}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    for word in words:\n",
    "        print(word)\n",
    "        \n",
    "       \n",
    "    \n",
    "def main(model, topic_number, N=20):\n",
    "    ind= find_ind_biggest(model, N, topic_number)\n",
    "    words= find_words(model, ind)\n",
    "    print_result(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topic = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS)),\n",
    "    ('dim-reduction', NMF(n_components=20, random_state=0)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', ...})),\n",
       "                ('dim-reduction', NMF(n_components=20, random_state=0))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_topic.fit(data_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2022 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ind_biggest(model, N, topic_number):\n",
    "    \"\"\"\n",
    "       Returns the indices that most contribute to a feature\n",
    "       model: trained ML model\n",
    "       N: The number of top words\n",
    "       topic_number: new feature index\n",
    "     \"\"\"\n",
    "    nmf= model.named_steps[\"dim-reduction\"]\n",
    "    return np.argsort(-nmf.components_[topic_number, :])[:N]\n",
    "\n",
    "\n",
    "\n",
    "def find_words(model, ind):\n",
    "    vectorizer= model.named_steps[\"vectorizer\"]\n",
    "\n",
    "    features= vectorizer.get_feature_names()\n",
    "\n",
    "    return [features[i] for i in ind]\n",
    "\n",
    "\n",
    "    \n",
    "def print_result(words, topic_number):\n",
    "    print (f\" Top words for topic: {topic_number}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    for word in words:\n",
    "        print(word)\n",
    "        \n",
    "       \n",
    "    \n",
    "def main(model, topic_number, N=20):\n",
    "    ind= find_ind_biggest(model, N, topic_number)\n",
    "    words= find_words(model, ind)\n",
    "    print_result(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "work\n",
      "perfectly\n",
      "for\n",
      "about\n",
      "8\n",
      "rewind\n",
      ".\n",
      " \n",
      "but\n",
      "once\n",
      "-PRON-\n",
      "eat\n",
      "one\n",
      "of\n",
      "-PRON-\n",
      "precious\n",
      "tape\n",
      "-PRON-\n",
      "will\n",
      "probably\n",
      "be\n",
      "as\n",
      "dissapointed\n",
      "as\n",
      "-PRON-\n",
      "be\n",
      ".\n",
      " \n",
      "Buyer\n",
      "beware\n",
      "...\n",
      "these\n",
      "rewinder\n",
      "be\n",
      "mostly\n",
      "the\n",
      "same\n",
      "look\n",
      ";\n",
      "but\n",
      "market\n",
      "by\n",
      "different\n",
      "company\n",
      ".\n",
      " \n",
      "-PRON-\n",
      "all\n",
      "eat\n",
      "tape\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(data_polarity[0]['reviewText']):\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(document):\n",
    "    return [word.lemma_.lower() for word in nlp(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_LEMMA = {word.lemma_.lower() for word in nlp(\" \".join(STOP_WORDS))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topic = Pipeline([\n",
    "    ('selector', KeySelector('reviewText')),\n",
    "   # ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS_LEMMA, tokenizer=lemmatize)),\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS)),\n",
    "    ('dim-reduction', NMF(n_components=20, random_state=0)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector', KeySelector(col='reviewText')),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', ...})),\n",
       "                ('dim-reduction', NMF(n_components=20, random_state=0))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_topic.fit(data_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ind_biggest(vector, num_words_topic):\n",
    "    \n",
    "    return np.argsort(-vector)[:num_words_topic]\n",
    "\n",
    "def find_words(model, ind):\n",
    "    vectorizer= model.named_steps[\"vectorizer\"]\n",
    "\n",
    "    features= vectorizer.get_feature_names()\n",
    "\n",
    "    return [features[i] for i in ind]\n",
    "\n",
    "\n",
    "    \n",
    "def print_result(words, topic_number):\n",
    "    print (f\" Top words for topic: {topic_number}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    for word in words:\n",
    "        print(word)\n",
    "        \n",
    "       \n",
    "\n",
    "    \n",
    "def find_words_of_topic(model, vector, topic_number, num_words_topic=20):\n",
    "    ind= find_ind_biggest(vector, num_words_topic)\n",
    "    words= find_words(model, ind)\n",
    "    print_result(words, topic_number)\n",
    "\n",
    "                      \n",
    "\n",
    "                      \n",
    "def main(model, num_words_topic=20):\n",
    "    for i, vector in enumerate(model.named_steps[\"dim-reduction\"].components_):\n",
    "        find_words_of_topic(model, vector, i,num_words_topic=num_words_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top words for topic: 0\n",
      "--------------------\n",
      "unit\n",
      "time\n",
      "buy\n",
      "bought\n",
      "amazon\n",
      "money\n",
      "don\n",
      "months\n",
      "working\n",
      "got\n",
      "worked\n",
      "item\n",
      "battery\n",
      "return\n",
      "thing\n",
      "waste\n",
      "new\n",
      "bad\n",
      "warranty\n",
      "years\n",
      " Top words for topic: 1\n",
      "--------------------\n",
      "lens\n",
      "canon\n",
      "lenses\n",
      "focus\n",
      "50mm\n",
      "hood\n",
      "sharp\n",
      "light\n",
      "nikon\n",
      "wide\n",
      "zoom\n",
      "shots\n",
      "images\n",
      "18\n",
      "macro\n",
      "low\n",
      "cap\n",
      "image\n",
      "prime\n",
      "70\n",
      " Top words for topic: 2\n",
      "--------------------\n",
      "headphones\n",
      "sound\n",
      "speakers\n",
      "ear\n",
      "bass\n",
      "pair\n",
      "ears\n",
      "volume\n",
      "music\n",
      "quality\n",
      "head\n",
      "headphone\n",
      "better\n",
      "comfortable\n",
      "hear\n",
      "noise\n",
      "sony\n",
      "like\n",
      "speaker\n",
      "set\n",
      " Top words for topic: 3\n",
      "--------------------\n",
      "cable\n",
      "tv\n",
      "monitor\n",
      "connect\n",
      "signal\n",
      "belkin\n",
      "extension\n",
      "modem\n",
      "needed\n",
      "length\n",
      "quality\n",
      "video\n",
      "foot\n",
      "connector\n",
      "printer\n",
      "picture\n",
      "long\n",
      "fine\n",
      "monster\n",
      "vga\n",
      " Top words for topic: 4\n",
      "--------------------\n",
      "camera\n",
      "pictures\n",
      "digital\n",
      "cameras\n",
      "canon\n",
      "batteries\n",
      "battery\n",
      "flash\n",
      "picture\n",
      "nikon\n",
      "memory\n",
      "remote\n",
      "photos\n",
      "zoom\n",
      "shoot\n",
      "kodak\n",
      "takes\n",
      "olympus\n",
      "tripod\n",
      "shots\n",
      " Top words for topic: 5\n",
      "--------------------\n",
      "dvd\n",
      "player\n",
      "cd\n",
      "play\n",
      "sony\n",
      "disc\n",
      "discs\n",
      "dvds\n",
      "players\n",
      "unit\n",
      "mp3\n",
      "tv\n",
      "recorder\n",
      "video\n",
      "memorex\n",
      "remote\n",
      "vcr\n",
      "disks\n",
      "rw\n",
      "cds\n",
      " Top words for topic: 6\n",
      "--------------------\n",
      "great\n",
      "price\n",
      "recommend\n",
      "worked\n",
      "highly\n",
      "need\n",
      "value\n",
      "perfect\n",
      "needed\n",
      "easy\n",
      "bought\n",
      "happy\n",
      "buy\n",
      "room\n",
      "nice\n",
      "definitely\n",
      "love\n",
      "years\n",
      "beat\n",
      "sturdy\n",
      " Top words for topic: 7\n",
      "--------------------\n",
      "mouse\n",
      "keyboard\n",
      "logitech\n",
      "trackball\n",
      "microsoft\n",
      "buttons\n",
      "mice\n",
      "hand\n",
      "button\n",
      "wheel\n",
      "optical\n",
      "wireless\n",
      "years\n",
      "switch\n",
      "click\n",
      "keys\n",
      "ball\n",
      "wrist\n",
      "like\n",
      "scroll\n",
      " Top words for topic: 8\n",
      "--------------------\n",
      "product\n",
      "recommend\n",
      "support\n",
      "amazon\n",
      "purchase\n",
      "advertised\n",
      "tech\n",
      "products\n",
      "excellent\n",
      "service\n",
      "purchased\n",
      "company\n",
      "expected\n",
      "described\n",
      "sure\n",
      "belkin\n",
      "reviews\n",
      "satisfied\n",
      "star\n",
      "description\n",
      " Top words for topic: 9\n",
      "--------------------\n",
      "radio\n",
      "antenna\n",
      "fm\n",
      "reception\n",
      "stations\n",
      "signal\n",
      "wire\n",
      "station\n",
      "better\n",
      "car\n",
      "radios\n",
      "static\n",
      "stereo\n",
      "terk\n",
      "channels\n",
      "speaker\n",
      "listen\n",
      "receiver\n",
      "small\n",
      "transmitter\n",
      " Top words for topic: 10\n",
      "--------------------\n",
      "card\n",
      "cards\n",
      "windows\n",
      "memory\n",
      "install\n",
      "cf\n",
      "software\n",
      "sandisk\n",
      "drivers\n",
      "quot\n",
      "xp\n",
      "pc\n",
      "sd\n",
      "video\n",
      "computer\n",
      "driver\n",
      "installed\n",
      "reader\n",
      "problems\n",
      "pci\n",
      " Top words for topic: 11\n",
      "--------------------\n",
      "router\n",
      "wireless\n",
      "linksys\n",
      "netgear\n",
      "network\n",
      "connection\n",
      "support\n",
      "internet\n",
      "firmware\n",
      "link\n",
      "tech\n",
      "modem\n",
      "signal\n",
      "setup\n",
      "routers\n",
      "problems\n",
      "minutes\n",
      "wired\n",
      "connect\n",
      "access\n",
      " Top words for topic: 12\n",
      "--------------------\n",
      "filter\n",
      "filters\n",
      "uv\n",
      "glass\n",
      "tiffen\n",
      "hoya\n",
      "protect\n",
      "lenses\n",
      "quality\n",
      "protection\n",
      "expensive\n",
      "nikon\n",
      "coated\n",
      "reflections\n",
      "threads\n",
      "multi\n",
      "clean\n",
      "polarizer\n",
      "flare\n",
      "glare\n",
      " Top words for topic: 13\n",
      "--------------------\n",
      "good\n",
      "quality\n",
      "price\n",
      "value\n",
      "build\n",
      "pretty\n",
      "sound\n",
      "far\n",
      "construction\n",
      "better\n",
      "best\n",
      "nice\n",
      "high\n",
      "solid\n",
      "looks\n",
      "deal\n",
      "cheap\n",
      "job\n",
      "excellent\n",
      "low\n",
      " Top words for topic: 14\n",
      "--------------------\n",
      "power\n",
      "usb\n",
      "drive\n",
      "computer\n",
      "device\n",
      "hub\n",
      "adapter\n",
      "plug\n",
      "port\n",
      "plugged\n",
      "devices\n",
      "cord\n",
      "ports\n",
      "drives\n",
      "surge\n",
      "switch\n",
      "pc\n",
      "windows\n",
      "needed\n",
      "supply\n",
      " Top words for topic: 15\n",
      "--------------------\n",
      "works\n",
      "like\n",
      "fine\n",
      "cord\n",
      "perfectly\n",
      "charm\n",
      "expected\n",
      "tv\n",
      "advertised\n",
      "needed\n",
      "issues\n",
      "remote\n",
      "plug\n",
      "exactly\n",
      "cheap\n",
      "need\n",
      "complaints\n",
      "right\n",
      "easy\n",
      "long\n",
      " Top words for topic: 16\n",
      "--------------------\n",
      "use\n",
      "easy\n",
      "34\n",
      "tv\n",
      "want\n",
      "time\n",
      "need\n",
      "set\n",
      "recommend\n",
      "love\n",
      "home\n",
      "laptop\n",
      "small\n",
      "little\n",
      "remote\n",
      "highly\n",
      "tripod\n",
      "nice\n",
      "lot\n",
      "able\n",
      " Top words for topic: 17\n",
      "--------------------\n",
      "bag\n",
      "case\n",
      "fit\n",
      "carry\n",
      "small\n",
      "strap\n",
      "fits\n",
      "tripod\n",
      "like\n",
      "nice\n",
      "laptop\n",
      "shoulder\n",
      "backpack\n",
      "size\n",
      "lenses\n",
      "space\n",
      "extra\n",
      "pocket\n",
      "cases\n",
      "pockets\n",
      " Top words for topic: 18\n",
      "--------------------\n",
      "work\n",
      "didn\n",
      "tried\n",
      "doesn\n",
      "windows\n",
      "remote\n",
      "software\n",
      "waste\n",
      "support\n",
      "money\n",
      "try\n",
      "fine\n",
      "tech\n",
      "adapter\n",
      "thing\n",
      "device\n",
      "worked\n",
      "box\n",
      "couldn\n",
      "different\n",
      " Top words for topic: 19\n",
      "--------------------\n",
      "cables\n",
      "quality\n",
      "hosa\n",
      "ordered\n",
      "monster\n",
      "connectors\n",
      "cheap\n",
      "wire\n",
      "belkin\n",
      "length\n",
      "job\n",
      "ethernet\n",
      "amazon\n",
      "audio\n",
      "network\n",
      "hdmi\n",
      "purchased\n",
      "male\n",
      "longer\n",
      "video\n"
     ]
    }
   ],
   "source": [
    "main(model_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
